{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caZYLROd8xnV"
      },
      "source": [
        "To teach a model to use your MCP server, click **Runtime** > **Run all**. Make sure you've enabled a free Tesla T4 GPU and edit the [configuration](#configuration) cell below!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [GitHub](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "**MCPâ€¢RL: Teach you agent how to use any MCP server**\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 3B model to effectively use any MCP server. Simply provide an MCP server url and the notebook will:\n",
        "\n",
        "1. Query the server's tools\n",
        "2. Generate a set of input tasks that use those tools\n",
        "3. Train the model on those tasks using automatic RULER evaluation\n",
        "4. Test the trained model by giving it new tasks to complete\n",
        "\n",
        "RULER judges response quality purely from the agent's final output - no labeled data required!\n",
        "\n",
        "*Note: In this notebook we use a local server, but the technique below applies to all MCP servers!*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OsrwCDQ5cviC"
      },
      "outputs": [],
      "source": [
        "# @title ðŸ’¿ Installation\n",
        "# Portions adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks)\n",
        "# Copyright (c) Unsloth contributors.\n",
        "# License: GNU LGPL v3.0.\n",
        "# Modifications by OpenPipe:\n",
        "# - switched to uv\n",
        "# - changed vllm/triton pinning logic\n",
        "# - added protobuf pins\n",
        "# - adjusted syntax for pushing to HF\n",
        "# See /licenses/LGPL-3.0.txt and /licenses/GPL-3.0.txt for full text.\n",
        "\n",
        "%%capture\n",
        "import os\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !uv pip install openpipe-art[backend]==0.5.9 tenacity fastmcp \"mcp>=1.11.0\" \"gql<4\" aiohttp --prerelease allow --no-cache-dir\n",
        "else:\n",
        "    try:\n",
        "        import numpy\n",
        "\n",
        "        get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except:\n",
        "        get_numpy = \"numpy\"\n",
        "    try:\n",
        "        import subprocess\n",
        "\n",
        "        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except:\n",
        "        is_t4 = False\n",
        "    get_vllm, get_triton = (\n",
        "        (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
        "    )\n",
        "    !uv pip install --upgrade \\\n",
        "        openpipe-art[backend]==0.4.11 tenacity fastmcp pillow==11.3.0 protobuf==5.29.5 {get_vllm} {get_numpy} --prerelease allow --no-cache-dir\n",
        "    !uv pip install -qqq {get_triton}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8b8kgQ69ZDM"
      },
      "source": [
        "<a name=\"configuration\"></a>\n",
        "\n",
        "### ðŸŽ¯ Configuration - Edit These Settings\n",
        "\n",
        "Add an OpenRouter API key below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "so6r1_OG9en3"
      },
      "outputs": [],
      "source": [
        "# Required - Used for generating training inputs and RULER evaluation\n",
        "OPENROUTER_API_KEY = \"\"  # Put your OpenRouter key here\n",
        "\n",
        "# ðŸ”Œ Point to any MCP server\n",
        "MCP_SERVER_URL = \"http://0.0.0.0:9000/mcp\"\n",
        "\n",
        "# Optional - Enables metric logging\n",
        "WANDB_API_KEY = \"\"\n",
        "\n",
        "# Choose the base model to train\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"  # Options: \"Qwen/Qwen2.5-3B-Instruct\", \"Qwen/Qwen2.5-7B-Instruct\", etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "I_AFDSOv_LrB"
      },
      "outputs": [],
      "source": [
        "# @title Advanced Settings\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"sql-agent-3b\"  # Name for your trained model\n",
        "PROJECT_NAME = \"mcp-rl\"  # Project name for tracking\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"num_training_inputs\": 16,  # Number of training inputs to generate\n",
        "    \"groups_per_step\": 2,  # Inputs to process per training step\n",
        "    \"num_epochs\": 1,  # Number of times through all data\n",
        "    \"rollouts_per_group\": 4,  # Different responses per input (for RULER comparison)\n",
        "    \"learning_rate\": 1e-5,  # Learning rate\n",
        "    \"max_training_steps\": None,  # Maximum training steps (set to None for no limit)\n",
        "}\n",
        "\n",
        "MAX_TURNS = 10  # Maximum number of turns for the model to generate during one rollout\n",
        "\n",
        "NUM_TEST_INPUTS = 8  # Number of test inputs to generate\n",
        "RULER_MODEL = \"openrouter/openai/gpt-4o-mini\"  # Model for RULER evaluation\n",
        "INPUT_GENERATION_MODEL = \"openai/gpt-4o-mini\"\n",
        "\n",
        "# Colab/T4 specific config to avoid OOM errors\n",
        "MAX_TURNS = 3  # Decrease the number of turns to avoid OOM errors on a T4\n",
        "MAX_SEQ_LENGTH = 8192  # Maximum sequence length\n",
        "GPU_MEMORY_UTILIZATION = 0.75  # GPU memory usage (0.0-1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "PfXGRuhhd7hr"
      },
      "outputs": [],
      "source": [
        "# @title Debug utilities\n",
        "\n",
        "import json\n",
        "import time\n",
        "import traceback\n",
        "from typing import Any\n",
        "\n",
        "DEBUG_LOG = True  # flip to False to silence logs\n",
        "LOG_JSON_MAX = 2000  # cap large JSON prints\n",
        "\n",
        "\n",
        "def _ts() -> str:\n",
        "    return time.strftime(\"%H:%M:%S\")\n",
        "\n",
        "\n",
        "def log(msg: str, **kv):\n",
        "    if not DEBUG_LOG:\n",
        "        return\n",
        "    parts = [f\"[{_ts()}] {msg}\"]\n",
        "    if kv:\n",
        "        kv_str = \" \".join(f\"{k}={repr(v)}\" for k, v in kv.items())\n",
        "        parts.append(\"| \" + kv_str)\n",
        "    print(\" \".join(parts))\n",
        "\n",
        "\n",
        "def log_json(title: str, payload: Any, max_len: int = LOG_JSON_MAX):\n",
        "    if not DEBUG_LOG:\n",
        "        return\n",
        "    try:\n",
        "        s = json.dumps(payload, indent=2, default=str)\n",
        "    except Exception:\n",
        "        s = str(payload)\n",
        "    if len(s) > max_len:\n",
        "        s = s[:max_len] + \"\\n... (truncated)\"\n",
        "    print(f\"[{_ts()}] {title}:\\n{s}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "leyy8bkHjSBc",
        "outputId": "672458b1-0460-4a27-accc-881d42668bcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mcp_server.py\n"
          ]
        }
      ],
      "source": [
        "# @title Create MCP server\n",
        "\n",
        "%%writefile mcp_server.py\n",
        "\"\"\"\n",
        "FastMCP SQLite Database Server\n",
        "A simple MCP server that exposes a company database for text-to-SQL agent training.\n",
        "\"\"\"\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "# Initialize in-memory SQLite database\n",
        "DB = sqlite3.connect(\":memory:\")\n",
        "DB.row_factory = sqlite3.Row\n",
        "\n",
        "DB.executescript(\"\"\"\n",
        "CREATE TABLE departments (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    name TEXT NOT NULL,\n",
        "    location TEXT NOT NULL,\n",
        "    budget REAL NOT NULL\n",
        ");\n",
        "\n",
        "CREATE TABLE employees (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    name TEXT NOT NULL,\n",
        "    department_id INTEGER REFERENCES departments(id),\n",
        "    role TEXT NOT NULL,\n",
        "    salary REAL NOT NULL,\n",
        "    hire_date TEXT NOT NULL\n",
        ");\n",
        "\n",
        "CREATE TABLE projects (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    name TEXT NOT NULL,\n",
        "    department_id INTEGER REFERENCES departments(id),\n",
        "    lead_id INTEGER REFERENCES employees(id),\n",
        "    status TEXT NOT NULL CHECK(status IN ('active', 'completed', 'on_hold')),\n",
        "    budget REAL NOT NULL\n",
        ");\n",
        "\n",
        "-- Departments\n",
        "INSERT INTO departments VALUES (1, 'Engineering',  'San Francisco', 2500000);\n",
        "INSERT INTO departments VALUES (2, 'Marketing',    'New York',      1200000);\n",
        "INSERT INTO departments VALUES (3, 'Data Science', 'London',        1800000);\n",
        "INSERT INTO departments VALUES (4, 'Sales',        'New York',       900000);\n",
        "INSERT INTO departments VALUES (5, 'Operations',   'San Francisco',  750000);\n",
        "\n",
        "-- Employees\n",
        "INSERT INTO employees VALUES (1,  'Alice Chen',      1, 'Senior Engineer',     145000, '2020-03-15');\n",
        "INSERT INTO employees VALUES (2,  'Bob Martinez',     1, 'Staff Engineer',      175000, '2018-07-01');\n",
        "INSERT INTO employees VALUES (3,  'Carol White',      2, 'Marketing Manager',   120000, '2019-11-20');\n",
        "INSERT INTO employees VALUES (4,  'David Kim',        3, 'Data Scientist',      135000, '2021-01-10');\n",
        "INSERT INTO employees VALUES (5,  'Eva Johnson',      1, 'Junior Engineer',      95000, '2023-06-01');\n",
        "INSERT INTO employees VALUES (6,  'Frank Brown',      4, 'Sales Lead',          110000, '2020-09-15');\n",
        "INSERT INTO employees VALUES (7,  'Grace Liu',        3, 'Senior Data Scientist',155000, '2019-04-22');\n",
        "INSERT INTO employees VALUES (8,  'Henry Wilson',     2, 'Content Strategist',   98000, '2022-02-14');\n",
        "INSERT INTO employees VALUES (9,  'Irene Davis',      5, 'Operations Manager',  115000, '2020-08-30');\n",
        "INSERT INTO employees VALUES (10, 'James Taylor',     1, 'Engineering Manager',  165000, '2017-05-12');\n",
        "INSERT INTO employees VALUES (11, 'Karen Patel',      3, 'ML Engineer',         140000, '2021-09-05');\n",
        "INSERT INTO employees VALUES (12, 'Leo Nguyen',       4, 'Account Executive',    92000, '2023-01-18');\n",
        "INSERT INTO employees VALUES (13, 'Maria Garcia',     5, 'Logistics Coordinator', 78000, '2022-07-25');\n",
        "INSERT INTO employees VALUES (14, 'Nathan Scott',     2, 'Brand Designer',      105000, '2021-03-11');\n",
        "INSERT INTO employees VALUES (15, 'Olivia Reed',      1, 'DevOps Engineer',     130000, '2020-12-01');\n",
        "\n",
        "-- Projects\n",
        "INSERT INTO projects VALUES (1, 'Cloud Migration',     1, 2,  'active',    500000);\n",
        "INSERT INTO projects VALUES (2, 'Brand Refresh',       2, 3,  'completed', 200000);\n",
        "INSERT INTO projects VALUES (3, 'Recommendation Engine',3, 7,  'active',    350000);\n",
        "INSERT INTO projects VALUES (4, 'Q4 Sales Push',       4, 6,  'active',    150000);\n",
        "INSERT INTO projects VALUES (5, 'Warehouse Automation', 5, 9,  'on_hold',   280000);\n",
        "INSERT INTO projects VALUES (6, 'ML Pipeline v2',      3, 11, 'active',    420000);\n",
        "INSERT INTO projects VALUES (7, 'Mobile App Redesign',  1, 10, 'active',    300000);\n",
        "INSERT INTO projects VALUES (8, 'SEO Overhaul',        2, 8,  'completed', 120000);\n",
        "\"\"\")\n",
        "\n",
        "import json\n",
        "from fastmcp import FastMCP\n",
        "\n",
        "# Create the MCP server\n",
        "mcp = FastMCP(\"company-db\", instructions=\"You are a database assistant. Use the tools to explore the database schema and run SQL queries to answer questions about the company data.\")\n",
        "\n",
        "# Tool 1: List all tables\n",
        "@mcp.tool()\n",
        "def list_tables() -> str:\n",
        "    \"\"\"List all tables in the database.\"\"\"\n",
        "    cursor = DB.execute(\n",
        "        \"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\"\n",
        "    )\n",
        "    tables = [row[\"name\"] for row in cursor.fetchall()]\n",
        "    return json.dumps(tables)\n",
        "\n",
        "\n",
        "# Tool 2: Describe a table's schema\n",
        "@mcp.tool()\n",
        "def describe_table(table_name: str) -> str:\n",
        "    \"\"\"Get the column names, types, and constraints for a specific table.\n",
        "\n",
        "    Args:\n",
        "        table_name: Name of the table to describe.\n",
        "    \"\"\"\n",
        "    # Validate table name to prevent injection\n",
        "    cursor = DB.execute(\n",
        "        \"SELECT name FROM sqlite_master WHERE type='table' AND name=?\",\n",
        "        (table_name,),\n",
        "    )\n",
        "    if not cursor.fetchone():\n",
        "        return json.dumps({\"error\": f\"Table '{table_name}' not found.\"})\n",
        "\n",
        "    columns = DB.execute(f\"PRAGMA table_info({table_name})\").fetchall()\n",
        "    schema = [\n",
        "        {\n",
        "            \"name\": col[\"name\"],\n",
        "            \"type\": col[\"type\"],\n",
        "            \"nullable\": not col[\"notnull\"],\n",
        "            \"primary_key\": bool(col[\"pk\"]),\n",
        "        }\n",
        "        for col in columns\n",
        "    ]\n",
        "    return json.dumps(schema, indent=2)\n",
        "\n",
        "\n",
        "# Tool 3: Run a SQL query\n",
        "@mcp.tool()\n",
        "def run_query(sql: str) -> str:\n",
        "    \"\"\"Execute a read-only SQL query and return the results.\n",
        "\n",
        "    Args:\n",
        "        sql: A SELECT SQL query to run against the database.\n",
        "    \"\"\"\n",
        "    # Block write operations\n",
        "    stripped = sql.strip().upper()\n",
        "    if not stripped.startswith(\"SELECT\"):\n",
        "        return json.dumps({\n",
        "            \"error\": \"Only SELECT queries are allowed.\"\n",
        "        })\n",
        "\n",
        "    try:\n",
        "        cursor = DB.execute(sql)\n",
        "        rows = [dict(row) for row in cursor.fetchall()]\n",
        "        return json.dumps({\"row_count\": len(rows), \"results\": rows}, indent=2)\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "\n",
        "# Run the server\n",
        "if __name__ == \"__main__\":\n",
        "    mcp.run(transport=\"streamable-http\", host=\"0.0.0.0\", port=9000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run the server\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"python\", \"mcp_server.py\"])"
      ],
      "metadata": {
        "id": "Sjf1YxUhlIa0",
        "outputId": "9044cdc6-46d5-4b0c-a174-95ee11f8bad6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['python', 'mcp_server.py']>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gxUn4E_IPjq8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "edb6463a-9e3e-4ba7-e5a5-22e89756031f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ExceptionGroup",
          "evalue": "unhandled errors in a TaskGroup (1 sub-exception)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mExceptionGroup\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1923110094.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mlist_tools_and_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tools:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m print(\n",
            "\u001b[0;32m/tmp/ipython-input-1923110094.py\u001b[0m in \u001b[0;36mlist_tools_and_resources\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlist_tools_and_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m\"\"\"Return (tools_result, resources_result) from the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmcp_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtools\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_tools\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0manext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopAsyncIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1923110094.py\u001b[0m in \u001b[0;36mmcp_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mConnects\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMCP\u001b[0m \u001b[0mserver\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mstreamablehttp_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMCP_SERVER_URL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mClientSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__aexit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mathrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopAsyncIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mcp/client/streamable_http.py\u001b[0m in \u001b[0;36mstreamablehttp_client\u001b[0;34m(url, headers, timeout, sse_read_timeout, terminate_on_close, httpx_client_factory, auth)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;31m# Manage client lifecycle since we created it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         async with streamable_http_client(\n\u001b[0m\u001b[1;32m    718\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0mhttp_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__aexit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mathrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopAsyncIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mcp/client/streamable_http.py\u001b[0m in \u001b[0;36mstreamable_http_client\u001b[0;34m(url, http_client, terminate_on_close)\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0mtransport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStreamableHTTPTransport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0manyio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Connecting to StreamableHTTP endpoint: {url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\u001b[0m in \u001b[0;36m__aexit__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    781\u001b[0m                     \u001b[0;31m# chaining and avoid adding a \"During handling of above...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m                     \u001b[0;31m# for each nesting level.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                     raise BaseExceptionGroup(\n\u001b[0m\u001b[1;32m    784\u001b[0m                         \u001b[0;34m\"unhandled errors in a TaskGroup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                     ) from None\n",
            "\u001b[0;31mExceptionGroup\u001b[0m: unhandled errors in a TaskGroup (1 sub-exception)"
          ]
        }
      ],
      "source": [
        "# @title ðŸ”Œ MCP helpers\n",
        "\n",
        "from contextlib import asynccontextmanager\n",
        "\n",
        "import mcp.types as types\n",
        "from mcp.client.session import ClientSession\n",
        "from mcp.client.streamable_http import streamablehttp_client\n",
        "\n",
        "if not MCP_SERVER_URL:\n",
        "    raise ValueError(\"MCP_SERVER_URL is empty. Set it in the Configuration cell.\")\n",
        "\n",
        "\n",
        "@asynccontextmanager\n",
        "async def mcp_session():\n",
        "    \"\"\"\n",
        "    Connects to the MCP server using the full URL.\n",
        "    \"\"\"\n",
        "    async with streamablehttp_client(MCP_SERVER_URL) as (read, write, _):\n",
        "        async with ClientSession(read, write) as session:\n",
        "            await session.initialize()\n",
        "            yield session\n",
        "\n",
        "\n",
        "async def list_tools_and_resources():\n",
        "    \"\"\"Return (tools_result, resources_result) from the server.\"\"\"\n",
        "    async with mcp_session() as session:\n",
        "        tools = await session.list_tools()\n",
        "        try:\n",
        "            resources = await session.list_resources()\n",
        "        except Exception:\n",
        "            # Some servers don't implement resources; keep interface stable\n",
        "            class _Empty:\n",
        "                resources = []\n",
        "\n",
        "            resources = _Empty()\n",
        "        return tools, resources\n",
        "\n",
        "\n",
        "async def call_mcp_tool(tool_name: str, arguments: dict):\n",
        "    \"\"\"Invoke a tool on the MCP server and return the CallToolResult.\"\"\"\n",
        "    async with mcp_session() as session:\n",
        "        return await session.call_tool(tool_name, arguments)\n",
        "\n",
        "\n",
        "tools, resources = await list_tools_and_resources()\n",
        "print(\"Tools:\", [t.name for t in tools.tools])\n",
        "print(\n",
        "    \"Resources:\",\n",
        "    [getattr(r, \"uri\", None) for r in getattr(resources, \"resources\", []) or []],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nEB1JGY6Pjq8"
      },
      "outputs": [],
      "source": [
        "# @title Let's generate our train and validation scenarios!\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Import the generate_scenarios function from art.mcp and logging utilities\n",
        "from art.mcp import generate_scenarios\n",
        "from art.mcp.generate_scenarios import preview_scenarios\n",
        "from art.utils.logging import info, ok, step, warn, err\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# required env/key check\n",
        "# If OPENROUTER_API_KEY exists as a var, use it; otherwise pull from env\n",
        "_openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "try:\n",
        "    _openrouter_key = _openrouter_key if _openrouter_key else OPENROUTER_API_KEY  # noqa: F821 (defined upstream in your notebook)\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "if _openrouter_key:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = _openrouter_key\n",
        "    ok(\"OPENROUTER_API_KEY found.\")\n",
        "else:\n",
        "    err(\"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\")\n",
        "    raise ValueError(\n",
        "        \"OPENROUTER_API_KEY is required for data generation and RULER evaluation.\"\n",
        "    )\n",
        "\n",
        "# Convert MCP tools and resources to the expected format\n",
        "tools_result, resources_result = await list_tools_and_resources()\n",
        "\n",
        "# Convert tools to the format expected by generate_scenarios\n",
        "tools_list = []\n",
        "for tool in tools_result.tools or []:\n",
        "    tools_list.append({\n",
        "        \"name\": tool.name,\n",
        "        \"description\": tool.description,\n",
        "        \"parameters\": tool.inputSchema,\n",
        "    })\n",
        "\n",
        "# Convert resources to the format expected by generate_scenarios\n",
        "resources_list = []\n",
        "for resource in getattr(resources_result, \"resources\", []) or []:\n",
        "    resources_list.append({\n",
        "        \"uri\": str(resource.uri),\n",
        "        \"name\": resource.name,\n",
        "        \"description\": resource.description,\n",
        "        \"mimeType\": resource.mimeType,\n",
        "    })\n",
        "\n",
        "# Calculate total scenarios needed\n",
        "try:\n",
        "    expected_total = TRAINING_CONFIG[\"num_training_inputs\"] + NUM_TEST_INPUTS  # noqa: F821\n",
        "except NameError:\n",
        "    err(\"TRAINING_CONFIG/NUM_TEST_INPUTS not defined in this notebook.\")\n",
        "    raise\n",
        "\n",
        "info(f\"Target total scenarios: {expected_total}\")\n",
        "\n",
        "# Generate scenarios using the art.mcp function\n",
        "max_attempts = 10\n",
        "scenarios = None\n",
        "\n",
        "for attempt in range(1, max_attempts + 1):\n",
        "    step(f\"Attempt {attempt}/{max_attempts} ...\")\n",
        "    t_attempt = time.perf_counter()\n",
        "    try:\n",
        "        scenario_collection = await generate_scenarios(\n",
        "            tools=tools_list,\n",
        "            resources=resources_list,\n",
        "            num_scenarios=expected_total,\n",
        "            show_preview=False,  # We'll preview separately for train/val\n",
        "            generator_model=INPUT_GENERATION_MODEL,\n",
        "            generator_api_key=_openrouter_key,\n",
        "        )\n",
        "        # Convert GeneratedScenarioCollection to list of dicts for compatibility\n",
        "        scenarios = [{\"task\": s.task, \"difficulty\": s.difficulty} for s in scenario_collection.scenarios]\n",
        "        ok(f\"Attempt {attempt} succeeded in {time.perf_counter() - t_attempt:.2f}s.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        warn(f\"Attempt {attempt} failed: {e}\")\n",
        "        if attempt < max_attempts:\n",
        "            time.sleep(min(1.5 * attempt, 6.0))\n",
        "        else:\n",
        "            err(\"All attempts exhausted.\")\n",
        "            raise\n",
        "\n",
        "# Split into train/val\n",
        "ok(f\"Generated {len(scenarios)} scenarios total.\")\n",
        "step(\"Shuffling scenarios and splitting into train/val ...\")\n",
        "random.shuffle(scenarios)\n",
        "\n",
        "train_n = TRAINING_CONFIG[\"num_training_inputs\"]  # noqa: F821\n",
        "raw_train_scenarios = scenarios[:train_n]\n",
        "raw_val_scenarios = scenarios[train_n:]\n",
        "\n",
        "ok(f\"Train: {len(raw_train_scenarios)} | Val: {len(raw_val_scenarios)}\")\n",
        "\n",
        "info(\"Sample (train) preview:\")\n",
        "preview_scenarios(raw_train_scenarios, n=min(5, len(raw_train_scenarios)))\n",
        "\n",
        "info(\"Sample (val) preview:\")\n",
        "preview_scenarios(raw_val_scenarios, n=min(5, len(raw_val_scenarios)))\n",
        "\n",
        "ok(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FET-_U0IPjq8"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to train your model!\n",
        "\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import weave\n",
        "from dotenv import load_dotenv\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "import art\n",
        "from art.local import LocalBackend\n",
        "from art.rewards import ruler_score_group\n",
        "from art.utils import iterate_dataset\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Optional\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "    weave.init(PROJECT_NAME)\n",
        "else:\n",
        "    print(\"WANDB_API_KEY is not set. We'll skip logging metrics to Weights & Biases.\")\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    name=MODEL_NAME,\n",
        "    project=PROJECT_NAME,\n",
        "    base_model=BASE_MODEL,\n",
        ")\n",
        "\n",
        "# To run on a T4, we need to override some config defaults.\n",
        "model._internal_config = art.dev.InternalModelConfig(\n",
        "    init_args=art.dev.InitArgs(\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "    ),\n",
        "    engine_args=art.dev.EngineArgs(\n",
        "        enforce_eager=True,\n",
        "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Initialize the server\n",
        "backend = LocalBackend(\n",
        "    in_process=True,\n",
        "    path=\"./.art\",\n",
        ")\n",
        "\n",
        "# Register the model with the local Backend\n",
        "await model.register(backend)\n",
        "\n",
        "print(\"Model created!\")\n",
        "print(\"Base model:\", BASE_MODEL)\n",
        "print(\"Model name:\", MODEL_NAME)\n",
        "print(\"Project name:\", PROJECT_NAME)\n",
        "\n",
        "\n",
        "def get_content_text(result) -> str:\n",
        "    # Extract text content from tool call result per MCP content schema\n",
        "    if isinstance(result, str):\n",
        "        return result\n",
        "    if hasattr(result, \"content\") and result.content:\n",
        "        out = \"\"\n",
        "        for item in result.content:\n",
        "            if isinstance(item, types.TextContent):\n",
        "                out += item.text\n",
        "            else:\n",
        "                out += str(item)\n",
        "        return out\n",
        "    if hasattr(result, \"structured\") and result.structured is not None:\n",
        "        try:\n",
        "            return json.dumps(result.structured)\n",
        "        except Exception:\n",
        "            return str(result.structured)\n",
        "    return str(result)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class McpScenario:\n",
        "    \"\"\"A scenario for MCP agent evaluation against a remote Smithery server.\"\"\"\n",
        "\n",
        "    task_description: str\n",
        "    max_turns: int = MAX_TURNS\n",
        "\n",
        "\n",
        "@weave.op()\n",
        "async def rollout(\n",
        "    model: art.Model,\n",
        "    scenario: McpScenario,\n",
        "    debug: bool = False,\n",
        ") -> art.Trajectory:\n",
        "    \"\"\"Run an MCP agent rollout against the remote Smithery MCP server.\"\"\"\n",
        "    traj = art.Trajectory(\n",
        "        messages_and_choices=[],\n",
        "        reward=0,\n",
        "        metadata={\"task\": scenario.task_description},\n",
        "        metrics={\n",
        "            \"task_completed\": False,\n",
        "            \"success\": False,\n",
        "            \"ran_out_of_turns\": False,\n",
        "        },\n",
        "        scenario=scenario,\n",
        "    )\n",
        "\n",
        "    # Discover available tools from the remote server\n",
        "    tools_result, _resources_result = await list_tools_and_resources()\n",
        "    tool_names = [t.name for t in tools_result.tools]\n",
        "    log(\"rollout: discovered tools\", count=len(tool_names), names=tool_names)\n",
        "\n",
        "    # Convert to OpenAI tool format\n",
        "    tool_schemas = []\n",
        "    for tool in tools_result.tools:\n",
        "        tool_schema = {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": tool.name,\n",
        "                \"description\": tool.description or f\"MCP tool: {tool.name}\",\n",
        "                \"parameters\": tool.inputSchema or {\"type\": \"object\", \"properties\": {}},\n",
        "            },\n",
        "        }\n",
        "        tool_schemas.append(tool_schema)\n",
        "\n",
        "    # Add completion tool schema\n",
        "    tool_schemas.append(\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"complete_task\",\n",
        "                \"description\": \"Complete the task with a summary\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"summary\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"Summary of accomplishments\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"summary\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    )\n",
        "\n",
        "    traj.tools = tool_schemas\n",
        "\n",
        "    # Initialize conversation\n",
        "    system_prompt = (\n",
        "        f\"You are an MCP (Model Context Protocol) agent.\\n\\n\"\n",
        "        f\"Use MCP tools through the server to complete your task.\\n\\n\"\n",
        "        f\"When you believe you have completed the task, call the 'complete_task' function with a summary of what you accomplished. \"\n",
        "        f\"You have a total of {scenario.max_turns} turns.\"\n",
        "        # NOTE: removing 'Only use tool calls, do not write any content.' â€” some models\n",
        "        # will freeze if they think plain text is disallowed. Let them output thoughts but\n",
        "        # we only process tool calls below.\n",
        "    )\n",
        "\n",
        "    traj.messages_and_choices = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Please complete this task: {scenario.task_description}\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    num_turns = 0\n",
        "    task_completed = False\n",
        "\n",
        "    # Main interaction loop\n",
        "    while num_turns < scenario.max_turns and not task_completed:\n",
        "        num_turns += 1\n",
        "\n",
        "        try:\n",
        "            # === Log request ===\n",
        "            last_user = next(\n",
        "                (m for m in reversed(traj.messages()) if m[\"role\"] == \"user\"), None\n",
        "            )\n",
        "            log(\n",
        "                \"LLM request\",\n",
        "                step=num_turns,\n",
        "                model=(model.inference_model_name or model.name),\n",
        "                tools=len(tool_schemas),\n",
        "                last_user=(last_user[\"content\"][:160] + \"...\" if last_user else None),\n",
        "            )\n",
        "\n",
        "            # Get LLM response\n",
        "            async with traj.track_duration(\"llm_completion\"):\n",
        "                openai_client = AsyncOpenAI(\n",
        "                    api_key=model.inference_api_key,\n",
        "                    base_url=model.inference_base_url,\n",
        "                )\n",
        "\n",
        "                # We also log the request body (without huge params)\n",
        "                req_preview = {\n",
        "                    \"model\": model.inference_model_name\n",
        "                    if model.inference_model_name\n",
        "                    else model.name,\n",
        "                    \"messages_len\": len(traj.messages()),\n",
        "                    \"tools_len\": len(tool_schemas),\n",
        "                }\n",
        "                log_json(\"LLM request (preview)\", req_preview)\n",
        "\n",
        "                response = await openai_client.chat.completions.create(\n",
        "                    model=model.inference_model_name\n",
        "                    if model.inference_model_name\n",
        "                    else model.name,\n",
        "                    messages=traj.messages(),\n",
        "                    tools=tool_schemas,\n",
        "                    max_completion_tokens=8000,\n",
        "                )\n",
        "\n",
        "            # === Log response ===\n",
        "            choice = response.choices[0]\n",
        "\n",
        "            finish_reason = getattr(choice, \"finish_reason\", None)\n",
        "            msg = choice.message\n",
        "            has_tools = bool(getattr(msg, \"tool_calls\", None))\n",
        "            content_preview = (\n",
        "                (msg.content[:200] + \"...\")\n",
        "                if isinstance(msg.content, str) and msg.content\n",
        "                else str(msg.content)[:200]\n",
        "            )\n",
        "            log(\n",
        "                \"LLM response parsed\",\n",
        "                finish_reason=finish_reason,\n",
        "                has_tool_calls=has_tools,\n",
        "                content_preview=content_preview,\n",
        "            )\n",
        "\n",
        "            traj.messages_and_choices.append(choice)\n",
        "\n",
        "            # Handle tool calls\n",
        "            if msg.tool_calls:\n",
        "                for tool_call in msg.tool_calls:\n",
        "                    try:\n",
        "                        log(\n",
        "                            \"Tool call received\",\n",
        "                            name=tool_call.function.name,\n",
        "                            raw_args=tool_call.function.arguments,\n",
        "                        )\n",
        "                        tool_args = json.loads(tool_call.function.arguments or \"{}\")\n",
        "\n",
        "                        if tool_call.function.name == \"complete_task\":\n",
        "                            traj.metrics[\"task_completed\"] = True\n",
        "                            task_completed = True\n",
        "                            traj.logs.append(\n",
        "                                f\"Task completion attempted with summary: {tool_args.get('summary', '')}\"\n",
        "                            )\n",
        "                            # We still append a tool message for completeness\n",
        "                            traj.messages_and_choices.append(\n",
        "                                {\n",
        "                                    \"role\": \"tool\",\n",
        "                                    \"tool_call_id\": tool_call.id,\n",
        "                                    \"content\": \"Task marked complete.\",\n",
        "                                }\n",
        "                            )\n",
        "                        else:\n",
        "                            # ðŸ”§ Call MCP tool through remote Smithery session\n",
        "                            result = await call_mcp_tool(\n",
        "                                tool_call.function.name, tool_args\n",
        "                            )\n",
        "\n",
        "                            content_text = get_content_text(result)\n",
        "                            log(\n",
        "                                \"Tool result\",\n",
        "                                name=tool_call.function.name,\n",
        "                                len=len(content_text),\n",
        "                            )\n",
        "\n",
        "                            if len(content_text) > 20000:\n",
        "                                # print(\n",
        "                                #     f\"Tool call result for {tool_call.function.name} is too long: {len(content_text)}\"\n",
        "                                # )\n",
        "                                # print(f\"Args: {tool_args}\")\n",
        "                                # print(content_text[:1000])\n",
        "                                # print(content_text[-1000:])\n",
        "                                raise Exception(\n",
        "                                    f\"Tool call result for {tool_call.function.name} is too long: {len(content_text)}\"\n",
        "                                )\n",
        "\n",
        "                            # Add tool response\n",
        "                            traj.messages_and_choices.append(\n",
        "                                {\n",
        "                                    \"role\": \"tool\",\n",
        "                                    \"tool_call_id\": tool_call.id,\n",
        "                                    \"content\": content_text,\n",
        "                                }\n",
        "                            )\n",
        "\n",
        "                    except Exception as e:\n",
        "                        traceback.print_exc()\n",
        "                        traj.logs.append(f\"Tool call error: {e}\")\n",
        "\n",
        "                        # Add error response\n",
        "                        traj.messages_and_choices.append(\n",
        "                            {\n",
        "                                \"role\": \"tool\",\n",
        "                                \"tool_call_id\": tool_call.id,\n",
        "                                \"content\": f\"Error: {str(e)}\",\n",
        "                            }\n",
        "                        )\n",
        "            else:\n",
        "                # No tool calls â€” log and continue (RULER will likely give 0)\n",
        "                log(\n",
        "                    \"LLM returned no tool_calls; skipping tool execution\",\n",
        "                    turn=num_turns,\n",
        "                )\n",
        "                # You can consider breaking here or letting it try another turn\n",
        "                # break\n",
        "\n",
        "        except Exception as e:\n",
        "            traceback.print_exc()\n",
        "            traj.logs.append(f\"Error in turn {num_turns}: {e}\")\n",
        "            break\n",
        "\n",
        "    if not task_completed and num_turns == scenario.max_turns:\n",
        "        traj.metrics[\"ran_out_of_turns\"] = True\n",
        "\n",
        "    traj.metrics[\"num_turns\"] = num_turns\n",
        "\n",
        "    return traj.finish()\n",
        "\n",
        "\n",
        "# =============== Training code ===============\n",
        "\n",
        "print(\n",
        "    f\"Using config: max_turns={MAX_TURNS}, rollouts_per_group={TRAINING_CONFIG['rollouts_per_group']}, \"\n",
        "    f\"groups_per_step={TRAINING_CONFIG['groups_per_step']}, num_epochs={TRAINING_CONFIG['num_epochs']}, \"\n",
        "    f\"learning_rate={TRAINING_CONFIG['learning_rate']}\"\n",
        ")\n",
        "\n",
        "await model.register(backend)\n",
        "\n",
        "train_scenarios = [\n",
        "    McpScenario(\n",
        "        task_description=scenario[\"task\"],\n",
        "        max_turns=MAX_TURNS,\n",
        "    )\n",
        "    for scenario in raw_train_scenarios\n",
        "]\n",
        "\n",
        "# Create dataset iterator using raw scenarios\n",
        "train_iterator = iterate_dataset(\n",
        "    train_scenarios,\n",
        "    groups_per_step=TRAINING_CONFIG[\"groups_per_step\"],\n",
        "    num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),  # Resume from checkpoint\n",
        ")\n",
        "\n",
        "# Main training loop using iterate_dataset\n",
        "for batch in train_iterator:\n",
        "    print(\"Gathering trajectory groups with RULER scoring...\")\n",
        "\n",
        "    # Use gather_trajectory_groups with ruler_score_group\n",
        "    groups = await art.gather_trajectory_groups(\n",
        "        (\n",
        "            art.TrajectoryGroup(\n",
        "                rollout(model, scenario, False)\n",
        "                for _ in range(TRAINING_CONFIG[\"rollouts_per_group\"])\n",
        "            )\n",
        "            for scenario in batch.items\n",
        "        ),\n",
        "        pbar_desc=f\"train gather step {batch.step}\",\n",
        "    )\n",
        "\n",
        "    scored_groups = []\n",
        "    for group in groups:\n",
        "        # Use RULER to assign relative scores to each trajectory\n",
        "        judged_group = await ruler_score_group(\n",
        "            group, judge_model=RULER_MODEL, debug=True, swallow_exceptions=True\n",
        "        )\n",
        "        scored_groups.append(judged_group)\n",
        "\n",
        "    print(\"starting train\")\n",
        "    await model.train(\n",
        "        scored_groups,\n",
        "        config=art.TrainConfig(learning_rate=TRAINING_CONFIG[\"learning_rate\"]),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YRO9ndqo5ky4"
      },
      "outputs": [],
      "source": [
        "# @title Test Your Model!\n",
        "\n",
        "# Generate test inputs\n",
        "print(\"Generating test inputs...\")\n",
        "val_scenarios = [\n",
        "    McpScenario(\n",
        "        task_description=scenario[\"task\"],\n",
        "        max_turns=MAX_TURNS,\n",
        "    )\n",
        "    for scenario in raw_val_scenarios\n",
        "]\n",
        "\n",
        "print(f\"\\nðŸ§ª Testing the trained model on {len(val_scenarios)} new inputs:\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, scenario in enumerate(val_scenarios):\n",
        "    print(f\"\\nTest {i + 1}:\")\n",
        "    print(f\"Input: {scenario.task_description}\")\n",
        "\n",
        "    # Run the model\n",
        "    result_trajectory = await rollout(model, scenario)\n",
        "\n",
        "    # Extract the model's response\n",
        "    messages = result_trajectory.messages()\n",
        "    model_response = messages[-1][\"content\"] if messages else \"No response\"\n",
        "\n",
        "    print(f\"Model output: {model_response}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nðŸŽ‰ Testing completed!\")\n",
        "print(\n",
        "    f\"\\nYour model '{MODEL_NAME}' has been trained to use the Smithery MCP server at:\"\n",
        ")\n",
        "print(SMITHERY_MCP_URL)\n",
        "print(\"\\nTo use this model in production:\")\n",
        "print(\"1. The model checkpoint is saved in ./.art/\")\n",
        "print(\"2. You can load it using the vLLM library\")\n",
        "print(\n",
        "    \"3. Or continue training with more examples by adjusting the configuration at the top\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "utI-VYM8s5lo"
      },
      "outputs": [],
      "source": [
        "# @title Upload to Hugging Face ðŸ¤—\n",
        "\n",
        "# Adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks), licensed under GNU LGPL v3.0.\n",
        "# Â© Unsloth contributors. Modifications Â© 2025 OpenPipe, Inc.\n",
        "# See THIRD-PARTY-NOTICES and licenses/LGPL-3.0.txt for details.\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "lora_model_path = (\n",
        "    f\".art/{model.project}/models/{model.name}/checkpoints/{await model.get_step():04d}\"\n",
        ")\n",
        "\n",
        "peft_model, peft_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=lora_model_path,\n",
        "    max_seq_length=16384,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "UPLOAD_MODEL = False  # Set True when you're ready to upload your model to Hugging Face\n",
        "HF_ACCOUNT = \"your_hf_account\"\n",
        "HF_TOKEN = \"your_hf_token\"\n",
        "\n",
        "if UPLOAD_MODEL:\n",
        "    peft_model.push_to_hub_merged(\n",
        "        f\"{HF_ACCOUNT}/{model.name}\", peft_tokenizer, token=HF_TOKEN\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuevYgXT-I1h"
      },
      "source": [
        "### Next Steps\n",
        "\n",
        "Congratulations! You've successfully trained a custom model for your task using only:\n",
        "- A pre-built MCP server\n",
        "- Example inputs (no outputs needed!)\n",
        "- RULER's automatic evaluation\n",
        "\n",
        "Here are some ways to improve results:\n",
        "\n",
        "1. **More diverse inputs**: Generate more varied input examples\n",
        "2. **Longer training**: Increase the number of training steps\n",
        "3. **More comparisons**: Increase `rollouts_per_group` for better RULER comparisons\n",
        "4. **MCP server refinement**: Add better tools and resources to the server\n",
        "5. **Hyperparameter tuning**: Adjust learning rate, batch size, etc.\n",
        "\n",
        "Remember: RULER learns what \"good\" means from your MCP server alone - no labeled data required!\n",
        "\n",
        "For more advanced use cases, check out the [ART documentation](https://art.openpipe.ai)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}